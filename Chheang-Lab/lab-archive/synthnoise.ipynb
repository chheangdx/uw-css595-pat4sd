{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snsynth import Synthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Block\n",
    "Data cannot be continuous to achieve DP-CTGAN. Anonymeter addresses this by binning each continous column into 50 equally spaced bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chhduong/Library/Python/3.9/lib/python/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "#pandas has a method for binning called cut:\n",
    "#binData = pd.cut(df['Data'], bins=50)\n",
    "dataframe = pd.read_parquet('datasets/adults_train.parquet')\n",
    "\n",
    "#all numerical columns are distributed into 50 bins labeled from 1 to 50\n",
    "\n",
    "# age\n",
    "bin_age = pd.cut(dataframe['age'], bins=50, labels=list(range(1,51)))\n",
    "dataframe['age'] = bin_age\n",
    "# fnlwgt\n",
    "bin_fnlwgt = pd.cut(dataframe['fnlwgt'], bins=50, labels=list(range(1,51)))\n",
    "dataframe['fnlwgt'] = bin_fnlwgt\n",
    "# education_num\n",
    "bin_education_num = pd.cut(dataframe['education_num'], bins=50, labels=list(range(1,51)))\n",
    "dataframe['education_num'] = bin_education_num\n",
    "# capital_gain\n",
    "bin_capital_gain = pd.cut(dataframe['capital_gain'], bins=50, labels=list(range(1,51)))\n",
    "dataframe['capital_gain'] = bin_capital_gain\n",
    "# capital_loss\n",
    "bin_capital_loss = pd.cut(dataframe['capital_loss'], bins=50, labels=list(range(1,51)))\n",
    "dataframe['capital_loss'] = bin_capital_loss\n",
    "# hr_per_week\n",
    "bin_hr_per_week = pd.cut(dataframe['hr_per_week'], bins=50, labels=list(range(1,51)))\n",
    "dataframe['hr_per_week'] = bin_hr_per_week\n",
    "\n",
    "#generate metadata\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(dataframe)\n",
    "metadata.visualize(\n",
    "    show_table_details='full',\n",
    "    output_filepath='datasets/adults_train_bin_metadata_stats.png'\n",
    ")\n",
    "dataframe.to_parquet('datasets/adults_train_bin.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP-CTGAN synethesization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chhduong/Library/Python/3.9/lib/python/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/chhduong/Library/Python/3.9/lib/python/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/chhduong/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.6679, Loss D: 1.3900\n",
      "epsilon is 0.17295241552213905, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.6655, Loss D: 1.3975\n",
      "epsilon is 0.1771795990317799, alpha is 63.0\n",
      "Epoch 3, Loss G: 0.6802, Loss D: 1.3820\n",
      "epsilon is 0.18140678254142079, alpha is 63.0\n",
      "Epoch 4, Loss G: 0.6840, Loss D: 1.3924\n",
      "epsilon is 0.18563396605106164, alpha is 63.0\n",
      "Epoch 5, Loss G: 0.6893, Loss D: 1.3835\n",
      "epsilon is 0.18986114956070252, alpha is 63.0\n",
      "Epoch 6, Loss G: 0.6888, Loss D: 1.3869\n",
      "epsilon is 0.19408833307034337, alpha is 63.0\n",
      "Epoch 7, Loss G: 0.6967, Loss D: 1.3950\n",
      "epsilon is 0.19831551657998422, alpha is 63.0\n",
      "Epoch 8, Loss G: 0.6957, Loss D: 1.3845\n",
      "epsilon is 0.2025427000896251, alpha is 63.0\n",
      "Epoch 9, Loss G: 0.6851, Loss D: 1.3893\n",
      "epsilon is 0.20676988359926596, alpha is 63.0\n",
      "Epoch 10, Loss G: 0.6835, Loss D: 1.3935\n",
      "epsilon is 0.2109970671089068, alpha is 63.0\n",
      "Epoch 11, Loss G: 0.6842, Loss D: 1.3821\n",
      "epsilon is 0.2152242506185477, alpha is 63.0\n",
      "Epoch 12, Loss G: 0.6859, Loss D: 1.3985\n",
      "epsilon is 0.21945143412818854, alpha is 63.0\n",
      "Epoch 13, Loss G: 0.6949, Loss D: 1.3986\n",
      "epsilon is 0.22367861763782942, alpha is 63.0\n",
      "Epoch 14, Loss G: 0.7129, Loss D: 1.3856\n",
      "epsilon is 0.22790580114747028, alpha is 63.0\n",
      "Epoch 15, Loss G: 0.7085, Loss D: 1.3738\n",
      "epsilon is 0.23213298465711113, alpha is 63.0\n",
      "Epoch 16, Loss G: 0.6957, Loss D: 1.3984\n",
      "epsilon is 0.23636016816675204, alpha is 63.0\n",
      "Epoch 17, Loss G: 0.6940, Loss D: 1.3956\n",
      "epsilon is 0.2405873516763929, alpha is 63.0\n",
      "Epoch 18, Loss G: 0.7025, Loss D: 1.3756\n",
      "epsilon is 0.24481453518603374, alpha is 63.0\n",
      "Epoch 19, Loss G: 0.6689, Loss D: 1.4158\n",
      "epsilon is 0.2490417186956746, alpha is 63.0\n",
      "Epoch 20, Loss G: 0.7339, Loss D: 1.3697\n",
      "epsilon is 0.25326890220531545, alpha is 63.0\n",
      "Epoch 21, Loss G: 0.7099, Loss D: 1.3808\n",
      "epsilon is 0.25749608571495636, alpha is 63.0\n",
      "Epoch 22, Loss G: 0.6859, Loss D: 1.4070\n",
      "epsilon is 0.2617232692245972, alpha is 63.0\n",
      "Epoch 23, Loss G: 0.7007, Loss D: 1.3882\n",
      "epsilon is 0.26595045273423806, alpha is 63.0\n",
      "Epoch 24, Loss G: 0.6832, Loss D: 1.4095\n",
      "epsilon is 0.2701776362438789, alpha is 63.0\n",
      "Epoch 25, Loss G: 0.6736, Loss D: 1.4058\n",
      "epsilon is 0.27440481975351977, alpha is 63.0\n",
      "Epoch 26, Loss G: 0.6206, Loss D: 1.4234\n",
      "epsilon is 0.2786320032631607, alpha is 63.0\n",
      "Epoch 27, Loss G: 0.7405, Loss D: 1.3499\n",
      "epsilon is 0.2828591867728015, alpha is 63.0\n",
      "Epoch 28, Loss G: 0.6678, Loss D: 1.3575\n",
      "epsilon is 0.2870863702824424, alpha is 63.0\n",
      "Epoch 29, Loss G: 0.6830, Loss D: 1.3763\n",
      "epsilon is 0.29131355379208324, alpha is 63.0\n",
      "Epoch 30, Loss G: 0.6568, Loss D: 1.4109\n",
      "epsilon is 0.2955407373017241, alpha is 63.0\n",
      "Epoch 31, Loss G: 0.6789, Loss D: 1.3887\n",
      "epsilon is 0.299767920811365, alpha is 63.0\n",
      "Epoch 32, Loss G: 0.6790, Loss D: 1.4010\n",
      "epsilon is 0.3039951043210058, alpha is 63.0\n",
      "Epoch 33, Loss G: 0.6843, Loss D: 1.3627\n",
      "epsilon is 0.3082222878306467, alpha is 63.0\n",
      "Epoch 34, Loss G: 0.6851, Loss D: 1.3259\n",
      "epsilon is 0.31244947134028755, alpha is 63.0\n",
      "Epoch 35, Loss G: 0.6443, Loss D: 1.3828\n",
      "epsilon is 0.3166766548499284, alpha is 63.0\n",
      "Epoch 36, Loss G: 0.7119, Loss D: 1.2606\n",
      "epsilon is 0.3209038383595693, alpha is 63.0\n",
      "Epoch 37, Loss G: 0.5743, Loss D: 1.4389\n",
      "epsilon is 0.3251310218692101, alpha is 63.0\n",
      "Epoch 38, Loss G: 0.6214, Loss D: 1.4035\n",
      "epsilon is 0.329358205378851, alpha is 63.0\n",
      "Epoch 39, Loss G: 0.6219, Loss D: 1.3926\n",
      "epsilon is 0.3335853888884919, alpha is 63.0\n",
      "Epoch 40, Loss G: 0.6206, Loss D: 1.4411\n",
      "epsilon is 0.3378125723981327, alpha is 63.0\n",
      "Epoch 41, Loss G: 0.6565, Loss D: 1.3861\n",
      "epsilon is 0.34203975590777363, alpha is 63.0\n",
      "Epoch 42, Loss G: 0.6695, Loss D: 1.3462\n",
      "epsilon is 0.34626693941741443, alpha is 63.0\n",
      "Epoch 43, Loss G: 0.6832, Loss D: 1.3439\n",
      "epsilon is 0.35049412292705534, alpha is 63.0\n",
      "Epoch 44, Loss G: 0.7462, Loss D: 1.2380\n",
      "epsilon is 0.3547213064366962, alpha is 63.0\n",
      "Epoch 45, Loss G: 0.6109, Loss D: 1.4097\n",
      "epsilon is 0.35894848994633705, alpha is 63.0\n",
      "Epoch 46, Loss G: 0.6718, Loss D: 1.3038\n",
      "epsilon is 0.36317567345597795, alpha is 63.0\n",
      "Epoch 47, Loss G: 0.4316, Loss D: 1.6854\n",
      "epsilon is 0.36740285696561875, alpha is 63.0\n",
      "Epoch 48, Loss G: 0.5588, Loss D: 1.4246\n",
      "epsilon is 0.37156280334051617, alpha is 62.0\n",
      "Epoch 49, Loss G: 0.4885, Loss D: 1.5504\n",
      "epsilon is 0.37569615878031204, alpha is 61.0\n",
      "Epoch 50, Loss G: 0.6703, Loss D: 1.3694\n",
      "epsilon is 0.3797885878969166, alpha is 61.0\n",
      "Epoch 51, Loss G: 0.7346, Loss D: 1.2671\n",
      "epsilon is 0.38383364375555507, alpha is 60.0\n",
      "Epoch 52, Loss G: 0.6518, Loss D: 1.3199\n",
      "epsilon is 0.3878587094749527, alpha is 60.0\n",
      "Epoch 53, Loss G: 0.5751, Loss D: 1.4495\n",
      "epsilon is 0.39182153470391107, alpha is 59.0\n",
      "Epoch 54, Loss G: 0.6272, Loss D: 1.3137\n",
      "epsilon is 0.395775974325623, alpha is 58.0\n",
      "Epoch 55, Loss G: 0.4862, Loss D: 1.5781\n",
      "epsilon is 0.39966634083652663, alpha is 58.0\n",
      "Epoch 56, Loss G: 0.6641, Loss D: 1.4053\n",
      "epsilon is 0.4035520108216999, alpha is 57.0\n",
      "Epoch 57, Loss G: 0.4981, Loss D: 1.5405\n",
      "epsilon is 0.4073750415162365, alpha is 57.0\n",
      "Epoch 58, Loss G: 0.5373, Loss D: 1.3917\n",
      "epsilon is 0.4111980722107731, alpha is 57.0\n",
      "Epoch 59, Loss G: 0.6034, Loss D: 1.3480\n",
      "epsilon is 0.41495512966637993, alpha is 56.0\n",
      "Epoch 60, Loss G: 0.5055, Loss D: 1.5399\n",
      "epsilon is 0.41871083373308526, alpha is 56.0\n",
      "Epoch 61, Loss G: 0.5823, Loss D: 1.4286\n",
      "epsilon is 0.4224146600650457, alpha is 55.0\n",
      "Epoch 62, Loss G: 0.7621, Loss D: 1.3072\n",
      "epsilon is 0.4261030466899173, alpha is 55.0\n",
      "Epoch 63, Loss G: 0.6203, Loss D: 1.4903\n",
      "epsilon is 0.42976230317663316, alpha is 54.0\n",
      "Epoch 64, Loss G: 0.5950, Loss D: 1.3938\n",
      "epsilon is 0.43338338154313544, alpha is 54.0\n",
      "Epoch 65, Loss G: 0.6499, Loss D: 1.3053\n",
      "epsilon is 0.4370044599096377, alpha is 54.0\n",
      "Epoch 66, Loss G: 0.5346, Loss D: 1.4592\n",
      "epsilon is 0.4405611845622412, alpha is 53.0\n",
      "Epoch 67, Loss G: 0.5042, Loss D: 1.4284\n",
      "epsilon is 0.4441149638513007, alpha is 53.0\n",
      "Epoch 68, Loss G: 0.5149, Loss D: 1.3835\n",
      "epsilon is 0.44764654511024976, alpha is 52.0\n",
      "Epoch 69, Loss G: 0.4307, Loss D: 1.6133\n",
      "epsilon is 0.45113303450026226, alpha is 52.0\n",
      "Epoch 70, Loss G: 0.5287, Loss D: 1.4742\n",
      "epsilon is 0.45461952389027477, alpha is 52.0\n",
      "Epoch 71, Loss G: 0.5970, Loss D: 1.3845\n",
      "epsilon is 0.458069579772134, alpha is 51.0\n",
      "Epoch 72, Loss G: 0.4321, Loss D: 1.6765\n",
      "epsilon is 0.4614887884389616, alpha is 51.0\n",
      "Epoch 73, Loss G: 0.5523, Loss D: 1.4314\n",
      "epsilon is 0.4649079971057891, alpha is 51.0\n",
      "Epoch 74, Loss G: 0.4735, Loss D: 1.5431\n",
      "epsilon is 0.4682883480749516, alpha is 50.0\n",
      "Epoch 75, Loss G: 0.5959, Loss D: 1.3517\n",
      "epsilon is 0.4716402851919255, alpha is 50.0\n",
      "Epoch 76, Loss G: 0.3572, Loss D: 1.7372\n",
      "epsilon is 0.47499222230889954, alpha is 50.0\n",
      "Epoch 77, Loss G: 0.3457, Loss D: 1.8255\n",
      "epsilon is 0.47831568775705574, alpha is 49.0\n",
      "Epoch 78, Loss G: 0.5465, Loss D: 1.4863\n",
      "epsilon is 0.481600362494977, alpha is 49.0\n",
      "Epoch 79, Loss G: 0.5901, Loss D: 1.3496\n",
      "epsilon is 0.4848850372328984, alpha is 49.0\n",
      "Epoch 80, Loss G: 0.5919, Loss D: 1.3228\n",
      "epsilon is 0.48816554338966345, alpha is 48.0\n",
      "Epoch 81, Loss G: 0.4512, Loss D: 1.5666\n",
      "epsilon is 0.491382964916805, alpha is 48.0\n",
      "Epoch 82, Loss G: 0.5575, Loss D: 1.3650\n",
      "epsilon is 0.49460038644394666, alpha is 48.0\n",
      "Epoch 83, Loss G: 0.8593, Loss D: 1.0664\n",
      "epsilon is 0.4978178079710882, alpha is 48.0\n",
      "Epoch 84, Loss G: 0.5246, Loss D: 1.3607\n",
      "epsilon is 0.5010032660226145, alpha is 47.0\n",
      "Epoch 85, Loss G: 0.3796, Loss D: 1.6514\n",
      "epsilon is 0.5041534435047179, alpha is 47.0\n",
      "Epoch 86, Loss G: 0.4874, Loss D: 1.5549\n",
      "epsilon is 0.5073036209868214, alpha is 47.0\n",
      "Epoch 87, Loss G: 0.4849, Loss D: 1.4414\n",
      "epsilon is 0.5104537984689249, alpha is 47.0\n",
      "Epoch 88, Loss G: 0.5739, Loss D: 1.4890\n",
      "epsilon is 0.513560749633785, alpha is 46.0\n",
      "Epoch 89, Loss G: 0.6301, Loss D: 1.3818\n",
      "epsilon is 0.5166436922340687, alpha is 46.0\n",
      "Epoch 90, Loss G: 0.6889, Loss D: 1.1254\n",
      "epsilon is 0.5197266348343524, alpha is 46.0\n",
      "Epoch 91, Loss G: 0.5252, Loss D: 1.3972\n",
      "epsilon is 0.5228095774346362, alpha is 46.0\n",
      "Epoch 92, Loss G: 0.4587, Loss D: 1.5745\n",
      "epsilon is 0.525856088388958, alpha is 45.0\n",
      "Epoch 93, Loss G: 0.4164, Loss D: 1.5991\n",
      "epsilon is 0.5288718052681117, alpha is 45.0\n",
      "Epoch 94, Loss G: 0.2539, Loss D: 1.9889\n",
      "epsilon is 0.5318875221472654, alpha is 45.0\n",
      "Epoch 95, Loss G: 0.4495, Loss D: 1.6115\n",
      "epsilon is 0.5349032390264192, alpha is 45.0\n",
      "Epoch 96, Loss G: 0.5315, Loss D: 1.5626\n",
      "epsilon is 0.5379090820070639, alpha is 44.0\n",
      "Epoch 97, Loss G: 0.5883, Loss D: 1.2989\n",
      "epsilon is 0.540857582323255, alpha is 44.0\n",
      "Epoch 98, Loss G: 0.2785, Loss D: 1.8652\n",
      "epsilon is 0.5438060826394461, alpha is 44.0\n",
      "Epoch 99, Loss G: 0.4038, Loss D: 1.4991\n",
      "epsilon is 0.5467545829556372, alpha is 44.0\n",
      "Epoch 100, Loss G: 0.3797, Loss D: 1.5707\n",
      "epsilon is 0.5497030832718283, alpha is 44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chhduong/Library/Python/3.9/lib/python/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataframe = pd.read_parquet('datasets/adults_train_bin.parquet')\n",
    "\n",
    "# dpctgan, patectgan\n",
    "synth = Synthesizer.create(\"dpctgan\", \n",
    "    generator_decay = (10**-5),\n",
    "    discriminator_decay = (10**-3), \n",
    "    batch_size = 64, \n",
    "    epochs = 100, \n",
    "    epsilon = 32, \n",
    "    verbose = True\n",
    ")\n",
    "synth.fit(dataframe, preprocessor_eps=1.0)\n",
    "dataframe_synth = synth.sample(1000)\n",
    "\n",
    "#note that the synthesizer does not have the ability to save/load\n",
    "\n",
    "dataframe_synth.to_parquet('datasets/adults_syn_dpctgan.parquet')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
